{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basketball challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import Basketball\n",
    "from DataMultiProcess import DataMultiProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 100\n",
    "split = 'training'\n",
    "\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Basketball\n",
    "dp = Basketball.Basketball(save_path, num_frames, split=split)\n",
    "dp.run('FFNN', testeverytrain=True, EPOCHS=EPOCHS)\n",
    "#dp.run('CNN3D', testeverytrain=True, EPOCHS=EPOCHS)\n",
    "#dp.run('CNN2DLSTM', testeverytrain=True, EPOCHS=EPOCHS)\n",
    "#dp.run('OPTICALCONV3D', testeverytrain=True, EPOCHS=EPOCHS, opticalpath=of_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#from IPython import display\n",
    "#import torchvision.transforms.functional as F\n",
    "#for batch, target in trainset:\n",
    "#    print(\"here is the \",target)\n",
    "#    view1 = batch[0]\n",
    "#    for idx, img in enumerate(view1):\n",
    "#        img1 = F.to_pil_image(img)\n",
    "#        display.clear_output(wait=True)\n",
    "#        display.display(img1.resize((320,240)), Image.NEAREST)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from IPython import display\n",
    "#import torchvision.transforms.functional as F\n",
    "#sets = DataLoader(dataset)\n",
    "#for bath, target in sets:\n",
    "#    print(\"here is the \",target)\n",
    "#    view1 = batch[0][0]\n",
    "#    for idx, img in enumerate(view1):\n",
    "#        img1 = F.to_pil_image(img)\n",
    "#        display.display(img1.resize((320,240)), Image.NEAREST)\n",
    "#        display.clear_output(wait=True)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize weights\n",
    "image preprocessing\n",
    "normalizing the inputs\n",
    "regularizations\n",
    "\n",
    "inceptio v4\n",
    "inceptionv4\n",
    "resnet 101\n",
    "resnet 152\n",
    "\n",
    "1x1 convolution\n",
    "\n",
    "efficient architectures\n",
    "How do we obtain networks that are computationally and memory efficient\n",
    "enough to run on mobile devices, possibly with limited power supply?\n",
    "\n",
    "Width multiplier:\n",
    "Reduce the number of channels in each layer. This has been\n",
    "called width multiplier in „MobileNets: Efficient Convolutional Neural Networks\n",
    "for Mobile Vision Applications“.\n",
    "What is the effect on the computational complexity and on the parameters?\n",
    "\n",
    "resolution multiplier \n",
    "Reduce the resolution of the input image. This has been called\n",
    "resolution multiplier in „MobileNets: Efficient Convolutional Neural Networks for\n",
    "Mobile Vision Applications“.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Direct approach\n",
    "Model based approach\n",
    "Model and learning-based approach\n",
    "\n",
    "https://paperswithcode.com/paper/quo-vadis-action-recognition-a-new-model-and\n",
    "\n",
    "1. [Implemented] Feed Forward Neural Network (FFNN)\n",
    "View 1 and View 2 are concatenated side by side at first and each of the samples are then also concatenated. In this way a vector \n",
    "is formed which is then feed to the Feed Forward Neural Network(FFNN). Other variation could be going deep with multiple hidden layers\n",
    "2. CNN2  [Implemented]\n",
    "    view1->conv3d\n",
    "    view2->conv3d-------------> these are somehow combined and \n",
    "    optical(view1) -> conv3d--> flatted to predict 2 classes\n",
    "    optical(view2) -> conv3d\n",
    "3. CNN3 [Implemented] (use LTSM) -> https://github.com/HHTseng/video-classification\n",
    "    encode each frame with conv2d and send the output to be decoded to a lstm\n",
    "4. CNN4 [Implemented]\n",
    "    view1 and view2 and concatinated side by side and feed through the conv3d and flattened feed through linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization: Adding a term to a loss function that penalizes for high weights. It tradies in some of the ability to fit the training data well for the ability to have a model generalize better to the data it has not seen before.\n",
    "E(Theta) = Sum of all inputs((target - input) ** 2 ) + (lamda/2*m) * weights**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalization:\n",
    "    It is a technique to ensure that architecture as well as the training data are chosen in such as way that the network makes a good prediction during training.\n",
    "    1. Validate: Without validation it is impossible to judge whether my model is reasonable or starts overfitting the data. \n",
    "    2. Agumentation.\n",
    "    3. Ensemble learning\n",
    "    4. Dropout\n",
    "    5. Regularization with a penalty\n",
    "    6. Data augmentation\n",
    "    7. Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import serialize\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "results_files_name=serialize.get_all_results_names()\n",
    "results_files_name = serialize.get_all_results_names()\n",
    "out = widgets.Output()\n",
    "\n",
    "w = widgets.Dropdown(\n",
    "    options=results_files_name,\n",
    "    description='Result',\n",
    ")\n",
    "def update(filename):\n",
    "    #f,ax1 = plt.subplots(figsize =(20,10))\n",
    "    data_from_csv = pd.read_csv(filename)\n",
    "    sns.pointplot(x='epochs',y='trainloss', data=data_from_csv)\n",
    "    sns.pointplot(x='epochs',y='testloss', color='red',data=data_from_csv)\n",
    "    plt.xlabel('Epocs',fontsize = 15,color='blue')\n",
    "    plt.ylabel('Loss',fontsize = 15,color='blue')\n",
    "    plt.title('Train/Test Performance',fontsize = 20,color='blue')\n",
    "    plt.text(45,0.60,'Train',color='blue',fontsize = 18)\n",
    "    plt.text(45,0.65,'Test',color='red',fontsize = 18)\n",
    "    display(w)\n",
    "    out\n",
    "\n",
    "\n",
    "def on_change(change):\n",
    "    if change['name'] == 'value' and (change['new'] != change['old']):\n",
    "        clear_output()\n",
    "        update(change['new'])\n",
    "\n",
    "w.observe(on_change)\n",
    "update(results_files_name[-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
