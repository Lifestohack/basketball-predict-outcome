{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basketball challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "from Dataset import Basketball\n",
    "from Dataprocess import Preprocess\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from Basketball import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run('FFNN', testeverytrain=True, EPOCHS=2)\n",
    "run('CNN3D', testeverytrain=True, EPOCHS=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from IPython import display\n",
    "#import torchvision.transforms.functional as F\n",
    "#sets = DataLoader(dataset)\n",
    "#for bath, target in sets:\n",
    "#    print(\"here is the \",target)\n",
    "#    view1 = batch[0][0]\n",
    "#    for idx, img in enumerate(view1):\n",
    "#        img1 = F.to_pil_image(img)\n",
    "#        display.display(img1.resize((320,240)), Image.NEAREST)\n",
    "#        display.clear_output(wait=True)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#TODO\n",
    "Initialize weights\n",
    "image preprocessing\n",
    "normalizing the inputs\n",
    "\n",
    "Direct approach\n",
    "Model based approach\n",
    "Model and learning-based approach\n",
    "\n",
    "https://paperswithcode.com/paper/quo-vadis-action-recognition-a-new-model-and\n",
    "\n",
    "\n",
    "1. Linear Regression\n",
    "2. Feed Forward Neural Network (FFNN)\n",
    "View 1 and View 2 are concatenated at first and each of the samples are then also concatenated. In this way a vector \n",
    "is formed which is then feed to the Feed Forward Neural Network(FFNN). Other variation could be going deep with multiple hidden layers\n",
    "3. CNN1 \n",
    "    view1 -> Conv3d\n",
    "    view2 -> Conv3d \n",
    "    after passing each view from the convolution network it is somehow combined and flatted to predict 2 classes\n",
    "4. CNN2\n",
    "    view1->conv3d\n",
    "    view2->conv3d-------------> these are somehow combined and \n",
    "    optical(view1) -> conv3d--> flatted to predict 2 classes\n",
    "    optical(view2) -> conv3d\n",
    "5. CNN3 (use LTSM) -> https://github.com/HHTseng/video-classification\n",
    "    predict next frames and again predict the two classes\n",
    "    View1->frame1->conv2d----> run each frame of view1 through the\n",
    "    View1->frame100->con2d---> conv2d layer and combine            -----> somehow combine and \n",
    "    View2->frame1->conv2d----> run each frame of view2 through the -----> flatted to predict 2 classes\n",
    "    View2->frame100->con2d---> conv2d layer and combine\n",
    "6. CNN4\n",
    "    view1 and view2 and concatinated through the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "#from torchvision.transforms.functional import to_tensor, to_grayscale\n",
    "#from PIL import Image\n",
    "#im_gray = output[0]\n",
    "#im_gray = im_gray[None, ...]\n",
    "#plt.imshow(im_gray[0, 0], vmin=0., vmax=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transformation = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.Resize((64,48)),\n",
    "    #torchvision.transforms.CenterCrop((48,48)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataprocess = Preprocess().background_subtractor\n",
    "\n",
    "path = \"data\"\n",
    "dataset = Basketball(path, split='training', num_frame = 100, img_transform = img_transformation, dataprocess=dataprocess)\n",
    "trainset, testset = dataset.train_test_split()\n",
    "\n",
    "trainset = DataLoader(trainset, shuffle=True)\n",
    "testset = DataLoader(testset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in trainset:\n",
    "    #inputs = inputs[0][0].unsqueeze(dim=0)\n",
    "    #inputs = inputs.view(1 , 3, 100, 48, 48)\n",
    "    break\n",
    "\n",
    "inputs = torch.cat((inputs[0][0], inputs[0][1]), 3)\n",
    "print(inputs.shape)\n",
    "#frame = inputs.shape[0]\n",
    "#channel = inputs.shape[1]\n",
    "#height = inputs.shape[2]\n",
    "#width = inputs.shape[3]\n",
    "#inputs = inputs.view(-1, channel, frame, height, width)\n",
    "#print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import torchvision.transforms.functional as F\n",
    "print(\"here is the \",target)\n",
    "\n",
    "for i in range(10):\n",
    "    for idx, img in enumerate(inputs):\n",
    "        img1 = F.to_pil_image(img)\n",
    "        display.display(img1.resize((320,240)), Image.NEAREST)\n",
    "        display.clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
