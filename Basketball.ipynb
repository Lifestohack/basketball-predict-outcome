{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basketball challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "from Dataset import Basketball\n",
    "from Dataprocess import Preprocess\n",
    "from modules import FFNN, CNN\n",
    "import random\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transformation = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((64,48)),\n",
    "    torchvision.transforms.CenterCrop((48,48)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprocess = Preprocess().background_subtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data\"\n",
    "dataset = Basketball(path, split='training', num_frame = 100, img_transform = img_transformation, dataprocess=dataprocess)\n",
    "trainset, testset = dataset.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = DataLoader(trainset, shuffle=True)\n",
    "testset = DataLoader(testset, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from IPython import display\n",
    "#import torchvision.transforms.functional as F\n",
    "#sets = DataLoader(dataset)\n",
    "#for batch, target in sets:\n",
    "#    print(\"here is the \",target)\n",
    "#    view1 = batch[0][0]\n",
    "#    for idx, img in enumerate(view1):\n",
    "#        img1 = F.to_pil_image(img)\n",
    "#        display.display(img1.resize((320,240)), Image.NEAREST)\n",
    "#        display.clear_output(wait=True)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#TODO\n",
    "Initialize weights\n",
    "image preprocessing\n",
    "normalizing the inputs\n",
    "\n",
    "Direct approach\n",
    "Model based approach\n",
    "Model and learning-based approach\n",
    "\n",
    "https://paperswithcode.com/paper/quo-vadis-action-recognition-a-new-model-and\n",
    "\n",
    "\n",
    "1. Linear Regression\n",
    "2. Feed Forward Neural Network (FFNN)\n",
    "View 1 and View 2 are concatenated at first and each of the samples are then also concatenated. In this way a vector \n",
    "is formed which is then feed to the Feed Forward Neural Network(FFNN). Other variation could be going deep with multiple hidden layers\n",
    "3. CNN1 \n",
    "    view1 -> Conv3d\n",
    "    view2 -> Conv3d \n",
    "    after passing each view from the convolution network it is somehow combined and flatted to predict 2 classes\n",
    "4. CNN2\n",
    "    view1->conv3d\n",
    "    view2->conv3d-------------> these are somehow combined and \n",
    "    optical(view1) -> conv3d--> flatted to predict 2 classes\n",
    "    optical(view2) -> conv3d\n",
    "5. CNN3 (use LTSM) -> https://github.com/HHTseng/video-classification\n",
    "    predict next frames and again predict the two classes\n",
    "    View1->frame1->conv2d----> run each frame of view1 through the\n",
    "    View1->frame100->con2d---> conv2d layer and combine            -----> somehow combine and \n",
    "    View2->frame1->conv2d----> run each frame of view2 through the -----> flatted to predict 2 classes\n",
    "    View2->frame100->con2d---> conv2d layer and combine\n",
    "6. CNN4\n",
    "    view1 and view2 and concatinated through the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dataset[0][0][0].shape)\n",
    "variables = dataset[0][0][0].numel()\n",
    "net = FFNN(variables, 2) # only 2 classifier hit or miss as output and variables input as parameters\n",
    "#variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channel = 1\n",
    "out_class = 2\n",
    "net = CNN() # only 2 classifier hit or miss as output and 1 channel as the shape of input will be (1,3,28x28)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torchvision.transforms.functional import to_tensor, to_grayscale\n",
    "from PIL import Image\n",
    "im_gray = output[0]\n",
    "\n",
    "im_gray = im_gray[None, ...]\n",
    "\n",
    "\n",
    "plt.imshow(im_gray[0, 0], vmin=0., vmax=1.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer, Loss, Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(),lr=0.001,momentum=0.4, nesterov=True)\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainset):\n",
    "    net.train()\n",
    "    loss_sum = 0\n",
    "    for batch, target in trainset:\n",
    "        #batch is taken only as view1\n",
    "        #change is later for both views\n",
    "        inputs = batch[0][0].view(1,-1).to(device)\n",
    "        #inputs = batch[0][0].unsqueeze(dim=0).to(device)\n",
    "        target = torch.as_tensor(target).to(device)\n",
    "        output = net(inputs)\n",
    "        net.zero_grad()\n",
    "        l = loss_fn(output, target)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += l.item()\n",
    "        #print(loss_sum)\n",
    "    print('\\nTrain set: {}, Average loss: {:.4f}\\n'.format(len(trainset), loss_sum/len(trainset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def test(net, testset):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, target in testset:\n",
    "            inputs = batch[0][0].view(1,-1).to(device)\n",
    "            target = torch.as_tensor(target).to(device)\n",
    "            output = net(inputs)\n",
    "            #print(output)\n",
    "            #print(torch.argmax(output))\n",
    "            loss_sum += loss_fn(output,target) / len(testset)\n",
    "            for _, prediction in enumerate(output):\n",
    "                if torch.argmax(prediction) == target:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    print('\\nTest set: {}, Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        len(testset), loss_sum, correct, len(testset), 100. * correct / len(testset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "for epoch in range(0, epochs):\n",
    "    train(net, trainset)\n",
    "    test(net, testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
